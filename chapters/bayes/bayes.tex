\chapterimage{chapter_head_1.png}

\chapter{Bayessche Netze}

\section{Intro}
Ein Bayessches Netz stellt die Wahrscheinlichkeiten von Ereignissen und deren Abhängigkeit (bzw. Unabhängigkeit) zueinander dar.

Bayessche Netze finden immer dort Anwendungsmöglichkeit, wo Logik und Unwissenheit aufeinandertreffen.
Sie dienen der Vereinfachung von komplexen Problemen mit Hilfe weniger stochastischer Regeln.

In der Praxis finden Bayessche Netze beispielsweise in der Spracherkennung, medizinischen Diagnose, Filtern von Spam, Bildverarbeitung, Analyse von Kaufverhalten und in vielen anderen Gebieten Anwendung.

\begin{figure}[h]
    \centering
    \includegraphics[width=.4\textwidth]{chapters/bayes/bayes_intro.pdf}
    \caption{Beispiel eines Bayesschen Netzes zur Diagnose von Krankheiten (ohne Wahrscheinlichkeiten)}
\end{figure}

\section{Grundlagen der Wahrscheinlichkeitsrechnung}
Einem Ereignis A weisen wir eine Wahrscheinlichkeit p(A) zwischen 0 (tritt nie ein) und 1 (tritt immer ein) zu.

Als marginal probability bezeichnen wir eine Wahrscheinlichkeit p(A), welche keine Abhängigkeiten aufweist.
Beispiel: A = Eine aus einem Skatdeck gezogene Karte ist Kreuz.
p(A) = 1/4 (oder 25~\%)

Als joint probability bezeichnen wir Wahrscheinlichkeiten, welche nebeneinander auftreten. p(A,B)
Beispiel: A (von oben) und B: Die Karte ist ein Bube

Da A und B voneinander unabhängige Ereignisse sind folgt:
p(A,B) = p(A) $\cdot$ p(B) = 1/8 $\cdot$ 1/4 = 1/32

Sind Ereignisse jedoch nicht unabhängig, so müssen wir die Wahrscheinlichkeit von p(A,B) bereits bestimmt haben.
Möchten wir nun für eine große Anzahl Ereignisse Wahrscheinlichkeiten berechnen, so benötigen wir extrem viele Daten.

\begin{center}
\begin{tabular}{ ccccc } \toprule
Sonne scheint & Werktag & Stau & Rasensprinkler & Tage im Jahr \\ \midrule
F & F & F & F & 4 \\
F & F & F & T & 5 \\
F & F & T & F & 2 \\
F & F & T & T & 1 \\
F & T & F & F & 13 \\
F & T & F & T & 2 \\
F & T & T & F & 66 \\
F & T & T & T & ... \\
T & F & F & F & ... \\
T & F & F & T & \\
T & F & T & F & \\
T & F & T & T & \\
T & T & F & F & \\
T & T & F & T & \\
T & T & T & F & \\
T & T & T & T & Errechenbar \\
\bottomrule
\end{tabular}
\end{center}


Bis auf 1 errechenbares Ergebnis benötigen wir die direkten Werte aller Kombinationen.
Wir benötigen also Für N Ereignisse $2^N-1$ Datensätze.
Gerade in der Medizin aus dem Anfangsbeispiel gibt es aber oft extrem viele Ereignisse.

Oftmals ist vieles davon aber auch nicht interessant, bzw. ableitbar.
Die Wahrscheinlichkeit, dass die Sonne scheint ist beispielsweise unabhängig davon ob der Tag ein Werktag ist.

Um die benötigten Datensätze möglichst gering zu halten betrachtet man bedingte Wahrscheinlichkeiten (conditional probability).

\section{Bedingte Wahrscheinlichkeit}
Bedingte Wahrscheinlichkeiten beruhen auf Annahmen.
Man nimmt etwa an, dass die Sonne keinen Einfluss auf die Frage hat, ob es nun ein Werktag ist, sehr wohl jedoch auf die Frage, ob der Sprinkler angeschaltet ist.
Gehen wir weiterhin davon aus, dass an sonnigen Tagen mehr Staus zustande kommen, da z.B. Fahrer geblendet werden.
Zudem gehen wir davon aus, dass werktags mehr Staus auftreten.
Fügen wir zudem noch das Verpassen des Essens hinzu, welches lediglich vom Stau abhängt.

\begin{figure}[h]
    \centering
    \includegraphics[width=.2\textwidth]{chapters/bayes/bayes_example.pdf}
\end{figure}

Unsere benötigten Datensätze mit fiktivem Inhalt:\\
$p(Sonne) = 0,8$\\
$p(Werktag)= 0,7$\\
$p(Rasensprinkler | Sonne) = 0,95$\\
$p(Rasensprinkler | !Sonne) = 0,20$\\
$p(Stau | Sonne,Werktag) = 0,8$\\
$p(Stau | !Sonne, Werktag) = 0,75$\\
$p(Stau | Sonne, !Werktag) = 0,15$\\
$p(Stau | !Sonne, !Werktag) = 0,05$\\
$p(Essen verpasst | Stau) = 0,5$\\
$p(Essen verpasst | !Stau) = 0,05$\\

Von 31 Datensätzen in einer Tabelle mit joint probabilities haben wir nun nur noch 10 benötigt Wahrscheinlichkeiten und können alle anderen nun problemlos errechnen. (Beispiel folgt)

Um bedingte Wahrscheinlichkeiten effektiv nutzen zu können benötigen wir einige Definitionen:
\begin{equation*}
p(A,B) = p (A | B) \cdot p (B)
\end{equation*}
Daraus folgt direkt:
\begin{equation*}
p(A,B,C) = p(A | B,C) \cdot p(B,C) = p(A | B,C) \cdot p(B | C) \cdot p(C)
\end{equation*}
Dies lässt sich für N Ereignisse wie folgt darstellen (Chain Rule):
\begin{equation*}
P\left(\bigcap_{k=1}^N A_k\right)  = \prod_{k=1}^N  \mathrm P\left(A_k \,\Bigg|\, \bigcap_{j=1}^{k-1} A_j\right)
\end{equation*}
Zudem bezeichnen wir A als von B unabhängig, wenn gilt:
\begin{equation*}
p(A | B) = p(A)
\end{equation*}
und A als von B unabhängig unter der Prämisse C, wenn gilt:
\begin{equation*}
p(A | C) \cdot p(B | C) = p(A,B | C)
\end{equation*}
%
Mit Hilfe der Chain-Rule können wir nun für das Beispiel von oben jede mögliche joint probability ausrechnen.
%
\subsection{Rechenbeispiel}
Wir suchen eine Möglichkeit folgenden Ausdruck zu berechnen:
\begin{equation*}
p(Essen ~verpasst,Stau,Werktag,Rasensprenger,Sonne)
\end{equation*}
Nach Anwendung der Chain Rule haben wir:
\begin{align*}
& p( Essen ~verpasst | Stau , Werktag, Essen ~verpasst, Sonne )\\
\cdot & p( Stau | Werktag , Rasensprinkler, Sonne )\\
\cdot & p( Werktag | Rasensprinkler, Sonne )\\
\cdot & p( Rasensprinkler | Sonne )\\
\cdot & p( Sonne )
\end{align*}
Wichtig ist hier den Aufbau des ersten Ausdrucks entlang der Abhängigkeiten zu formulieren. Die Reihenfolge der Ereignisse für die joint probabilty muss also eine umgedrehte topologische Anordnung unseres Graphen darstellen bzw. ein Ereignis A kann nicht vor einem Ereignis B stehen, wenn B abhängig von A ist.\\
Nach dem Kürzen der Unabhängigkeiten erhalten wir:
\begin{align*}
p(Essen ~verpasst | Stau ) 
\cdotp( Stau | Werktag, Sonne)
\cdot p(Werktag) 
\cdot p(Rasensprinkler | Sonne)
\cdot p(Sonne)
\end{align*}
Man nennt dies auch die Faktorisierung.\\
Die gesuchten Wahrscheinlichkeiten kennen wir bereits und können sie einsetzen:
\begin{equation*}
0,5 \cdot 0,8 \cdot 0,7 \cdot 0,95 \cdot 0,8 = 0,218.
\end{equation*}
%
Es ist also durch simples Einsetzen unserer 10 Werte möglich sämtliche 31 Werte für die joint probability Tabelle zu berechnen.
Gleichzeitig sind Lösungen für relevante Fragen zur bedingten Wahrscheinlichkeit, welche nicht alle Ereignisse umfassen, mit geringem Aufwand zu lösen.
%
In der Regel sind jedoch nicht alle Angaben bekannt.
Nehmen wir nun an, dass wir als hart arbeitender Mensch an einem Nicht-Werktag zur Arbeit fahren und es sonnig ist.
Wir möchten nun wissen, wie wahrscheinlich es ist, dass man es abends bei der Rückfahrt rechtzeitig zum Essen schafft.
Wir suchen also:
\begin{equation*}
p(!Essen ~verpasst | !Werktag, Sonne)
\end{equation*}
Dazu berechnen wir:\\
\begin{equation*}
p(\!Essen ~verpasst | Stau) \cdot p(Stau | Sonne, \!Werktag) + p(\!Essen ~verpasst | \!Stau)
\cdot p(\!Stau | Sonne, \!Werktag)
\end{equation*}
%
Die passenden Werte kennen wir entweder oder können sie direkt ableiten:
\begin{align*}
&p(Essen ~verpasst | Stau) = 0,5 \rightarrow p(\!Essen ~verpasst | Stau) = 0.5\\
&p(Essen ~verpasst | \!Stau) = 0,05 \rightarrow p(\!Essen ~verpasst | \!Stau) = 0.95\\
&p(Stau | Sonne, \!Werktag) = 0,15 \rightarrow p(\!Stau | Sonne, \!Werktag) = 0.85\\\\
&0,5 \cdot 0,15 + 0,95 \cdot 0,85 = 0,8825 
\end{align*}
%
bzw. die Wahrscheinlichkeit am Wochenende pünktlich zum Essen zu kommen beträgt 88\%.\\
Was jedoch, wenn ich in die andere Richtung rechnen möchte?
Beispiel: Der Lebensgefährte kommt pünktlich zum Essen und ich möchte wissen, wie wahrscheinlich es ist, dass er im Stau gesteckt hat.
%
Wir kennen bereits die Formel:
\begin{equation*}
p(A,B) = p (A | B) \cdot p(B)
\end{equation*}
offensichtlich gilt auch:
\begin{align*}
&p(B,A) = p (B | A) \cdot p(A) \\
\Rightarrow & p(A | B) = p(A,B) / P(B) = p(B,A) / P(B) = p(B | A) \cdot p(A)/p(B) ~\text{(Satz von Bayes)}
\end{align*}
%
Wir wissen also:
\begin{align*}
&p(Stau | !Essen ~verpasst) = p(!Essen ~verpasst | Stau ) *P(Stau)/p(! Essen ~verpasst)\\
&p(!Essen ~verpasst | Stau) =0,5\\
p(Stau) =&p(Stau | Sonne, Werktag) \cdot p(Sonne) \cdot p(Werktag) + \\
&p(Stau | Sonne, !Werktag) \cdot p(Sonne) \cdot p(!Werktag)+ \\
&p(Stau | !Sonne, Werktag) \cdot p(!Sonne) \cdot p(Werktag) + \\
&p(Stau | !Sonne, !Werktag) \cdot p(!Sonne) \cdot p(!Werktag)= 0,382\\
&p(!Essen ~verpasst) =\\
&p(!Essen ~verpasst | Stau) \cdot p(Stau) + p(!Essen ~verpasst | !Stau) \cdot p(!Stau) = 0,7781 \\
&\Rightarrow p(Stau | !Essen ~verpasst)=0,5 \cdot 0,382/0,7781 = 0,2454...
\end{align*}
%
Ist die Person also pünktlich, so gab es mit einer Wahrscheinlichkeit von ~75\% keinen Stau.
%
An dieser Stelle ist anzufügen, dass das ganze Problem auch so hätte modelliert werden können, dass die Pünktlichkeit beim Essen ebenfalls vom Werktag abhängig ist, dann jedoch wäre die Abhängigkeit zum Stau zu hinterfragen, wenn es denn keinen Werktag gibt.
Es gibt also durchaus komplexere Problemstellungen, die wir mit unseren einfachen Methoden nicht so einfach behandeln können und eventuell verschiedene Modelle für Abhängigkeiten die je nach der Menge der Daten möglicherweise nicht optimal sind.
%
Diese neue Methode hilft uns vor allen Dingen damit mit einem einzelnen Modell gleichzeitig z.B. Krankheiten und Symptome in beide Richtungen zu bestimmen. Ohne große Umstände kann eine Krankheit aus Symptomen bestimmt werden und direkt von der Wahrscheinlichkeit der Krankheit kann die Wahrscheinlichkeit für weitere Symptome berechnet werden.
%
\section{Graphische Darstellung}
\begin{figure}[h]
    \centering
    \includegraphics[width=.2\textwidth]{chapters/bayes/bayes_net_1.pdf}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=.2\textwidth]{chapters/bayes/bayes_net_2.pdf}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=.2\textwidth]{chapters/bayes/bayes_net_3.pdf}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=.2\textwidth]{chapters/bayes/bayes_net_4.pdf}
\end{figure}

Wir nennen zwei Ereignisse X und Y bedingt unabhängig gegeben E, wenn im Graphen kein Weg von X zu Y existiert, welcher nicht E nicht beinhaltet.
Dies wir auch d-Separation genannt.
Wir schreiben hierfür
$X \perp Y | E$
Ausgehend vom vorherigen Beispiel heißt das, dass das Verpassen des Essens zwar vom Werktag abhängt, jedoch wenn bekannt ist, ob es einen Stau gab, diese Abhängigkeit aufgehoben wird.
Für 3 verknüpfte Ereignisse gibt es die Folgenden Möglichkeiten:
\begin{enumerate}[label=(\alph*)]
\item $A \rightarrow B \rightarrow C$ impliziert $A \perp C | B$
\item $A \leftarrow B \leftarrow C$ impliziert $A \perp C | B$
\item $A \leftarrow B \rightarrow C$ impliziert $A \perp C | B$
\item $A \rightarrow B \leftarrow C$ impliziert $A \perp C$
\end{enumerate}
Das Beispiel (d) beschreibt hierbei eine sogenannte V-Struktur.

Betrachten wir hierzu außerdem die Verschiedenen Faktorisierungen:
\begin{enumerate}[label=(\alph*)]
\item p(A,B,C) = p(C|B) $\cdot$ p(B|A) $\cdot$ p(A)
\item p(A,B,C) = p(A|B) $\cdot$ p(B|C) $\cdot$ p(C)
\item p(A,B,C) = p(A|B) $\cdot$ p(C|B) $\cdot$ p(B)
\item p(A,B,C) = p(B|A,C) $\cdot$ p(A) $\cdot$ p(C)
\end{enumerate}

\section{IC-Algorithmus}
Der IC Algorithmus ist eine Möglichkeit aus gegebenen Unabhängigkeiten ein Bayessches Netz zu erstellen.
\begin{enumerate}
\item Konstruiere einen ungerichteten Graphen; füge jede mögliche Kante (X,Y), für die es keine (bedingte) Abhängigkeit $X \perp Y | E$ bzw. $X \perp Y$ gibt, in den Graphen ein.
\item Gilt für zwei Nachbarn X,Y von einem Knoten E die bedingte Unabhängigkeit $X \perp Y | E$ nicht, dann füge die Kanten (X,E) und (Y,E) hinzu. (V-Struktur)
\item Orientiere die verbleibenden Kanten beliebig, aber ohne neue V-Strukturen entstehen zu lassen.
\end{enumerate}

\section{Erstellung eines Bayesschen Netzes}
Nach unseren theoretischen Überlegungen sind wir zur Erkenntnis gelangt, dass uns Bayessche Netze nicht nur ein intuitives Verständnis bzgl.
bedingten Wahrscheinlichkeiten vermitteln, indem Abhängigkeitsverhältnisse zwischen Zufallsvariablen explizit angegeben werden, sondern
auch langwierige Rechnungen erheblich vereinfachen können. Allerdings stellt sich für uns immer noch die Frage, wie wir aus einem
probabilistischen Problem ein geeignetes Netz entwickeln können.


\subsection{Aufbau eines Bayesschen Netzes aus Expertenwissen}   
Eine Möglichkeit wäre die Hilfe von Experten in Anspruch zu nehmen. Ihre Aufgabe besteht darin Abhängigkeitsverhältnisse anzugeben und
somit eine Topologie zu entwickeln sowie die entsprechenden Parameter der Knoten anzupassen. Diese Herangehensweise ist jedoch meist
nur für kleine bzw. unveränderliche Probleme geeignet, da sie zahlreiche Schwierigkeiten mit sich bringt.
Zum einen existiert ein Komplexitätsproblem. Offensichtlich kann allein die schiere Anzahl von Zufallsvariablen diesen Lösungsversuch
zunichtemachen. Aber wie ist es mit einer begrenzten, übersichtlichen Anzahl an Zufallsvariablen? 

Erinnern wir uns an die Kanten des Graphen. Jede Kante beschreibt eine Wahrscheinlichkeit und  kann einen Wert zwischen 0 und 1 annehmen.
Somit ist jede Wahrscheinlichkeit maximal eine gute Approximation der Realität. Da es oft der Fall ist, dass jede Zufallsvariable von
mehreren Eltern abhängt und ihrerseits die Wahrscheinlichkeit ihrer Kinder bestimmt, können bereits kleinste Fehler zu großem Schaden
führen und das Bayes Netz somit unbrauchbar machen. 
Dies lässt die Schlussfolgerung zu, dass ein konstruiertes Netz von verschiedenen Experten auf Fehler untersucht werden sollte, sodass
der ganze Entwicklungsprozess sehr langwierig wird. Dennoch kann das Ergebnis nicht verifiziert werden.

\subsection{Aufbau eines Bayesschen Netzes aus selbstlernenden Algorithmen} 
Da der Aufbau eines Bayesschen Netzes aus Expertenwissen einige Probleme mit sich bringt, haben Wissenschaftler eine weitere Methode
entwickelt. Sie besteht darin, dass Netz in die Lage zu versetzen, anfallende Daten zu analysieren und auf Grundlage des Ergebnis
Justierungen vorzunehmen. Da solche Algorithmen sehr komplex sind, werden im folgenden nur die Grundzüge der Verfahren vorgestellt
und einhergehende Schlüsselwörter genannt. Grundsätzlich gibt es zwei Möglichkeiten ein Netz zu erstellen bzw. zu verbessern.

\subsubsection{Anpassung der Parameter}
Ein Experte erstellt eine sinnvolle Topologie und fügt Parameter ein. Diese Parameter beruhen auf A-priori Wahrscheinlichkeiten und 
sollten eine gute Approximation darstellen. Solch ein Netz wird „augmented Bayesian network“ genannt. Die Idee besteht darin,
dass die Werte der Parameter kontinuierlich an die anfallenden Daten angepasst werden. Dies hat zur Folge, dass dasNetz umso bessere
Vorhersagen treffen kann, je mehr Daten bereits verarbeitet wurden. In diesem Kontext spielt besonders dierelative Häufigkeit
eine wichtige Rolle. Darunter versteht man wie oft ein gewisses Ereignis in einer Zeitspanne eintrat.\\
Beispiel: Wir werfen eine Münze. Wir schätzen, dass die Wahrscheinlichkeit 50 \% beträgt, dass das Ereignis Kopf eintritt. Nach dem 
10.000 Wurf stellen wir fest, dass 7000 mal das Ereignis Kopf eingetreten ist. Somit beträgt die relative Häufigkeit 70 \% und wir
müssen unseren anfangs gewählten Wert anpassen. \\
Diese Art von Berechnung ist jedoch nur möglich, wenn die relative Häufigkeit eines Parameters mit  [0,1] gleich wahrscheinlich ist. 
In vielen Fällen muss jedoch ein anderer Weg eingeschlagen werden, da diese Art von Verteilung nicht angenommen werden kann. Somit
benötigen wir eine Verteilungsfunktion, die in der Lage ist solche Verteilungen darzustellen.
Zu diesem Zweck bietet sich besonders die Familie der Beta Wahrscheinlichkeitsdichtefunktionen an, da diese über die Eigenschaft
verfügen, nach Dateneingang dynamisch und kontinuierlich angepasst werden zu können. 

\subsubsection{Erstellung der Topologie}
Die zweite Möglichkeit ein Bayes Netz durch Algorithmen konfigurieren zu lassen, besteht darin die Struktur eines Bayesschen
Netzes entsprechend der Datenlage anzupassen.
Eine populäre Methode ist der „score-based approach“. Dabei wird jeder Struktur ein Score zugewiesen, der angeben soll, wie geeignet
gegebene Daten durch eine beliebige Struktur beschrieben werden. Der Score wird folgendermaßen berechnet:\\
\\
P(G|D) = Score(G, D),
mithilfe der Bayesschen Formel zu

\begin{equation}
 P(G|D) = \frac{{P(D|G) * P(G)}}{{P(D)}}
\end{equation}

Da uns die zur Verfügung gestellten Daten nicht weiter interessieren, konzentrieren wir uns einzig auf den Zähler, da wir das Ziel
haben diesen Ausdruck zu maximieren.
P(G) kann durch verschiedene Möglichkeiten mithilfe von „prior information“ bestimmt werden.
Prior information geben Wahrscheinlichkeitsverteilungen an,  bevor es Beweise für deren Richtigkeit gibt. Sie beruhen meist auf
früheren Beobachtungen oder entstehen durch Vergleich mit ähnlichen Problemen.
Somit liegt der Fokus in der Maximierung von P(D|G). Es wurden viele verschiedene Möglichkeiten entwickelt dieses Problem zu
lösen, eine Standardmethode beinhaltet meistens die Likelihood-Berechnung, ein Schätzverfahren, dass die Maximierung eines
Ausdrucks anstrebt durch  Schätzen der Parameter anstrebt. 



\subsection{Vorteile von Bayesschen Netzen}			
Bayessche Netze ersparen ihren Anwender enorm viel Arbeit. Sie reduzieren die benötigten Wahrscheinlichkeitstabellen auf ein Minimum,
sodass manche Anwendungen laufzeit-technisch erst möglich werden.
Beispielsweise benötigt die Berechnung der Joint probability eine Laufzeit aus O($2^{n}$), unter der Bedingung, dass die gewählten
Zufallsvariablen binär sind, mithilfe von unserem Netz eine Laufzeit aus O($2^{pa(x)}$) wobei pa(x) für die Anzahl aller Eltern eines
Knotens x steht. 

Ein weiterer Vorteil von einem Bayes Netz liegt darin, dass der Aufbau des Netzes eine übersichtliche grafische Darstellung
ermöglicht und im Gegensatz zu neuronalen Netzen nicht als Black-Boxen für viele Benutzer wahrgenommen werden. Es wird durch sie
intuitiv erkennbar, welche Abhängigkeiten bestehen und mit welcher Wahrscheinlichkeit Ereignisse eintreten werden. Besonders für
Fachfremde wird das Verständnis enorm verbessert, was direkte Folgen für die Lösung der Probleme hat. Ein Arzt wird der Interpretation
von Symptomen einer Künstlichen Intelligenz mehr Vertrauen schenken, wenn er weiß, wie sie funktioniert.

Außerdem sollte nicht unterschätzt werden, dass ein bereits angelegtes Bayessches Netz nicht auf einen einzigen Typus einer
Problemstellung reduziert werden muss. Nennen wir als Beispiel ein Netz, dass die verschiedenen Mikrocontroller innerhalb eines
PKWs verknüpft, sodass diese im Zusammenspiel entscheiden können, wann die automatische Notbremsung eingeleitet werden soll.
Produziert man nun ein ähnliches Modell mit einer unterschiedlichen Bremsvorrichtung, so muss man lediglich das Subnetz der
Bremsen kappen um das neue System eingliedern zu können.   
Dadurch können Bayessche Netze als flexibel interpretiert werden, da man bis zu einem bestimmtem Grad das System an seine 
Bedürfnisse anpassen kann.

\subsection{Nachteile von Bayesschen Netzen}			
In der Praxis treten einige Probleme mit Bayesschen Netzen auf. In unseren theoretischen Überlegungen finden wir das Axiom
der zyklenfreiheit unseres Graphen. Dies ist jedoch oft nicht vorhersagbar, da unbekannte Abhängigkeiten zwischen Zufallsvariablen
existieren können. 

Es treten insbesondere vermehrt Probleme auf, wenn das Netz selbst lernend ist.
Zum einen besteht die Chance von Manipulation, wenn es von User-Inputs abhängt. Gibt ein User gezielt gefälschte Daten ein, 
so kann es passieren, dass die Wahrscheinlichkeitsberechnungen der einzelnen Knoten gestört werden, was zur Folge hat, dass 
auch ihre Kind-Knoten beeinflusst werden können. Da oft mehrere verschiedene Kind-Knoten pro Knoten existieren,  geben diese
dann ihrerseits den Input weiter, sodass oft ein großer Teil des Netzes manipulierbar ist.

Auch der automatische Aufbau der Topologie erweist sich als problematisch. Um die best mögliche Anordnung zu erreichen, muss 
jede Anordnung berechnet werden. Dies kann sehr zeitintensiv werden und bei einer höheren Anzahl an Zufallsvariablen unter 
Umständen unmöglich. Dieses Problem ist NP-schwer.

\section{Quellen}
\begin{itemize}
\item Open Course „Artifical Intelligence“ am MIT
\item https://upload.wikimedia.org/math/b/5/a/b5a87dba9ec79dd6a93628c85fab ca97.png
\item http://www.informatik.uni-bremen.de/tdki/lehre/ss12/bayes/Intro.pdf
\item http://www.fil.ion.ucl.ac.uk/~wpenny/bdb/bayes.pdf
\item Bayesian Artificial Intelligence, Second Edition
\item https://www.cs.cmu.edu/~dmarg/Papers/PhD-Thesis-Margaritis.pdf
\item https://en.wikipedia.org/wiki/Prior\_probability
\item http://www.markmeloon.com/some-advantages-of-bayesian-networks/
\item http://niedermayer.ca/book/export/html/29
\item http://www.cs.technion.ac.il/~dang/books/Learning\%20Bayesian\%20Networks(Neapolitan,\%20Richard).pdf
\end{itemize}
